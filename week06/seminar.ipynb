{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seminar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhrn5O-qUYZ"
      },
      "source": [
        "# Import and misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meO-Mp9jiAFC",
        "outputId": "e10c2c2d-6b11-4e11-8e52-49bb4662b24a"
      },
      "source": [
        "!pip install torchaudio==0.9.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==0.9.1\n",
            "  Downloading torchaudio-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 4.2 MB/s \n",
            "\u001b[?25hCollecting torch==1.9.1\n",
            "  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 7.1 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.1->torchaudio==0.9.1) (3.10.0.2)\n",
            "Installing collected packages: torch, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.9.1 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.9.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.1 torchaudio-0.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbUpoArCqUYa"
      },
      "source": [
        "from typing import Tuple, Union, List, Callable, Optional\n",
        "from tqdm import tqdm\n",
        "from itertools import islice\n",
        "import pathlib\n",
        "import dataclasses\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import distributions\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torchaudio\n",
        "from IPython import display as display_"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "812GwLfqqUYf"
      },
      "source": [
        "# Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1DuQIyRqUYf"
      },
      "source": [
        "In this notebook we will implement a model for finding a keyword in a stream.\n",
        "\n",
        "We will implement the version with CRNN because it is easy and improves the model. \n",
        "(from https://www.dropbox.com/s/22ah2ba7dug6pzw/KWS_Attention.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PdhApeEh9pH"
      },
      "source": [
        "@dataclasses.dataclass\n",
        "class TaskConfig:\n",
        "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
        "    batch_size: int = 128\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 1e-5\n",
        "    num_epochs: int = 20\n",
        "    n_mels: int = 40\n",
        "    cnn_out_channels: int = 8\n",
        "    kernel_size: Tuple[int, int] = (5, 20)\n",
        "    stride: Tuple[int, int] = (2, 8)\n",
        "    hidden_size: int = 64\n",
        "    gru_num_layers: int = 2\n",
        "    bidirectional: bool = False\n",
        "    num_classes: int = 2\n",
        "    sample_rate: int = 16000\n",
        "    device: torch.device = torch.device(\n",
        "        'cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA1gPmE1h9pI"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2N8zcx9MF1X"
      },
      "source": [
        "# !wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
        "# !mkdir speech_commands && tar -C speech_commands -xvzf speech_commands_v0.01.tar.gz 1> log"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12wBTK0mNUsG"
      },
      "source": [
        "class SpeechCommandDataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        transform: Optional[Callable] = None,\n",
        "        path2dir: str = None,\n",
        "        keywords: Union[str, List[str]] = None,\n",
        "        csv: Optional[pd.DataFrame] = None\n",
        "    ):        \n",
        "        self.transform = transform\n",
        "\n",
        "        if csv is None:\n",
        "            path2dir = pathlib.Path(path2dir)\n",
        "            keywords = keywords if isinstance(keywords, list) else [keywords]\n",
        "            \n",
        "            all_keywords = [\n",
        "                p.stem for p in path2dir.glob('*')\n",
        "                if p.is_dir() and not p.stem.startswith('_')\n",
        "            ]\n",
        "\n",
        "            triplets = []\n",
        "            for keyword in all_keywords:\n",
        "                paths = (path2dir / keyword).rglob('*.wav')\n",
        "                if keyword in keywords:\n",
        "                    for path2wav in paths:\n",
        "                        triplets.append((path2wav.as_posix(), keyword, 1))\n",
        "                else:\n",
        "                    for path2wav in paths:\n",
        "                        triplets.append((path2wav.as_posix(), keyword, 0))\n",
        "            \n",
        "            self.csv = pd.DataFrame(\n",
        "                triplets,\n",
        "                columns=['path', 'keyword', 'label']\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.csv = csv\n",
        "    \n",
        "    def __getitem__(self, index: int):\n",
        "        instance = self.csv.iloc[index]\n",
        "\n",
        "        path2wav = instance['path']\n",
        "        wav, sr = torchaudio.load(path2wav)\n",
        "        wav = wav.sum(dim=0)\n",
        "        \n",
        "        if self.transform:\n",
        "            wav = self.transform(wav)\n",
        "\n",
        "        return {\n",
        "            'wav': wav,\n",
        "            'keywors': instance['keyword'],\n",
        "            'label': instance['label']\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1rVkT81Pk90"
      },
      "source": [
        "dataset = SpeechCommandDataset(\n",
        "    path2dir='speech_commands', keywords=TaskConfig.keyword\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DFwhAXdfQLIA",
        "outputId": "12fe03cd-ecd9-4f5a-e285-5396670c7cc1"
      },
      "source": [
        "dataset.csv.sample(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>keyword</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>64550</th>\n",
              "      <td>speech_commands/stop/39543cfd_nohash_0.wav</td>\n",
              "      <td>stop</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35300</th>\n",
              "      <td>speech_commands/tree/80fe1dc7_nohash_0.wav</td>\n",
              "      <td>tree</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15380</th>\n",
              "      <td>speech_commands/bed/1cb788bc_nohash_1.wav</td>\n",
              "      <td>bed</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23513</th>\n",
              "      <td>speech_commands/one/c1d39ce8_nohash_8.wav</td>\n",
              "      <td>one</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13830</th>\n",
              "      <td>speech_commands/seven/333784b7_nohash_0.wav</td>\n",
              "      <td>seven</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              path keyword  label\n",
              "64550   speech_commands/stop/39543cfd_nohash_0.wav    stop      0\n",
              "35300   speech_commands/tree/80fe1dc7_nohash_0.wav    tree      0\n",
              "15380    speech_commands/bed/1cb788bc_nohash_1.wav     bed      0\n",
              "23513    speech_commands/one/c1d39ce8_nohash_8.wav     one      0\n",
              "13830  speech_commands/seven/333784b7_nohash_0.wav   seven      0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUxfDJw1qUYi"
      },
      "source": [
        "### Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmkxPWQqUYe"
      },
      "source": [
        "class AugsCreation:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.background_noises = [\n",
        "            'speech_commands/_background_noise_/white_noise.wav',\n",
        "            'speech_commands/_background_noise_/dude_miaowing.wav',\n",
        "            'speech_commands/_background_noise_/doing_the_dishes.wav',\n",
        "            'speech_commands/_background_noise_/exercise_bike.wav',\n",
        "            'speech_commands/_background_noise_/pink_noise.wav',\n",
        "            'speech_commands/_background_noise_/running_tap.wav'\n",
        "        ]\n",
        "\n",
        "        self.noises = [\n",
        "            torchaudio.load(p)[0].squeeze()\n",
        "            for p in self.background_noises\n",
        "        ]\n",
        "\n",
        "    def add_rand_noise(self, audio):\n",
        "\n",
        "        # randomly choose noise\n",
        "        noise_num = torch.randint(low=0, high=len(\n",
        "            self.background_noises), size=(1,)).item()\n",
        "        noise = self.noises[noise_num]\n",
        "\n",
        "        noise_level = torch.Tensor([1])  # [0, 40]\n",
        "\n",
        "        noise_energy = torch.norm(noise)\n",
        "        audio_energy = torch.norm(audio)\n",
        "        alpha = (audio_energy / noise_energy) * \\\n",
        "            torch.pow(10, -noise_level / 20)\n",
        "\n",
        "        start = torch.randint(\n",
        "            low=0,\n",
        "            high=max(int(noise.size(0) - audio.size(0) - 1), 1),\n",
        "            size=(1,)\n",
        "        ).item()\n",
        "        noise_sample = noise[start: start + audio.size(0)]\n",
        "\n",
        "        audio_new = audio + alpha * noise_sample\n",
        "        audio_new.clamp_(-1, 1)\n",
        "        return audio_new\n",
        "\n",
        "    def __call__(self, wav):\n",
        "        aug_num = torch.randint(low=0, high=4, size=(1,)).item()   # choose 1 random aug from augs\n",
        "        augs = [\n",
        "            lambda x: x,\n",
        "            lambda x: (x + distributions.Normal(0, 0.01).sample(x.size())).clamp_(-1, 1),\n",
        "            lambda x: torchaudio.transforms.Vol(.25)(x),\n",
        "            lambda x: self.add_rand_noise(x)\n",
        "        ]\n",
        "\n",
        "        return augs[aug_num](wav)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClWThxyYh9pM"
      },
      "source": [
        "indexes = torch.randperm(len(dataset))\n",
        "train_indexes = indexes[:int(len(dataset) * 0.8)]\n",
        "val_indexes = indexes[int(len(dataset) * 0.8):]\n",
        "\n",
        "train_df = dataset.csv.iloc[train_indexes].reset_index(drop=True)\n",
        "val_df = dataset.csv.iloc[val_indexes].reset_index(drop=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDPLht5fqUYe"
      },
      "source": [
        "# Sample is a dict of utt, word and label\n",
        "train_set = SpeechCommandDataset(csv=train_df, transform=AugsCreation())\n",
        "val_set = SpeechCommandDataset(csv=val_df)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmrJd8WIhkLP"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vbPDqd6qUYj"
      },
      "source": [
        "### Sampler for oversampling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfnjRKo2qUYj"
      },
      "source": [
        "# We should provide to WeightedRandomSampler _weight for every sample_; by default it is 1/len(target)\n",
        "\n",
        "def get_sampler(target):\n",
        "    class_sample_count = np.array(\n",
        "        [len(np.where(target == t)[0]) for t in np.unique(target)])   # for every class count it's number of occ.\n",
        "    weight = 1. / class_sample_count\n",
        "    samples_weight = np.array([weight[t] for t in target])\n",
        "    samples_weight = torch.from_numpy(samples_weight)\n",
        "    samples_weigth = samples_weight.float()\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "    return sampler"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM8gLmHeqUYj"
      },
      "source": [
        "train_sampler = get_sampler(train_set.csv['label'].values)\n",
        "val_sampler = get_sampler(val_set.csv['label'].values)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyBqbxp0h9pO"
      },
      "source": [
        "class Collator:\n",
        "    \n",
        "    def __call__(self, data):\n",
        "        wavs = []\n",
        "        labels = []    \n",
        "\n",
        "        for el in data:\n",
        "            wavs.append(el['wav'])\n",
        "            labels.append(el['label'])\n",
        "\n",
        "        # torch.nn.utils.rnn.pad_sequence takes list(Tensors) and returns padded (with 0.0) Tensor\n",
        "        wavs = pad_sequence(wavs, batch_first=True)    \n",
        "        labels = torch.Tensor(labels).long()\n",
        "        return wavs, labels"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8G9xPRVqUYk"
      },
      "source": [
        "###  Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wGBMcQiqUYk"
      },
      "source": [
        "# Here we are obliged to use shuffle=False because of our sampler with randomness inside.\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=TaskConfig.batch_size,\n",
        "                          shuffle=False, collate_fn=Collator(),\n",
        "                          sampler=train_sampler,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "val_loader = DataLoader(val_set, batch_size=TaskConfig.batch_size,\n",
        "                        shuffle=False, collate_fn=Collator(),\n",
        "                        sampler=val_sampler,\n",
        "                        num_workers=2, pin_memory=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlsn6cpqUYk"
      },
      "source": [
        "### Creating MelSpecs on GPU for speeeed: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRXMt6it56fW"
      },
      "source": [
        "class LogMelspec:\n",
        "\n",
        "    def __init__(self, is_train, config):\n",
        "        # with augmentations\n",
        "        if is_train:\n",
        "            self.melspec = nn.Sequential(\n",
        "                torchaudio.transforms.MelSpectrogram(\n",
        "                    sample_rate=config.sample_rate,\n",
        "                    n_fft=400,\n",
        "                    win_length=400,\n",
        "                    hop_length=160,\n",
        "                    n_mels=config.n_mels\n",
        "                ),\n",
        "                torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
        "                torchaudio.transforms.TimeMasking(time_mask_param=35),\n",
        "            ).to(config.device)\n",
        "\n",
        "        # no augmentations\n",
        "        else:\n",
        "            self.melspec = torchaudio.transforms.MelSpectrogram(\n",
        "                sample_rate=config.sample_rate,\n",
        "                n_fft=400,\n",
        "                win_length=400,\n",
        "                hop_length=160,\n",
        "                n_mels=config.n_mels\n",
        "            ).to(config.device)\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # already on device\n",
        "        return torch.log(self.melspec(batch).clamp_(min=1e-9, max=1e9))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqkz4_gn8BiF"
      },
      "source": [
        "melspec_train = LogMelspec(is_train=True, config=TaskConfig)\n",
        "melspec_val = LogMelspec(is_train=False, config=TaskConfig)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoAxmihY8yxr"
      },
      "source": [
        "### Quality measurment functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euwD1UyuqUYk"
      },
      "source": [
        "# FA - true: 0, model: 1\n",
        "# FR - true: 1, model: 0\n",
        "\n",
        "def count_FA_FR(preds, labels):\n",
        "    FA = torch.sum(preds[labels == 0])\n",
        "    FR = torch.sum(labels[preds == 0])\n",
        "    \n",
        "    # torch.numel - returns total number of elements in tensor\n",
        "    return FA.item() / torch.numel(preds), FR.item() / torch.numel(preds)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHBUrkT1qUYk"
      },
      "source": [
        "def get_au_fa_fr(probs, labels):\n",
        "    sorted_probs, _ = torch.sort(probs)\n",
        "    sorted_probs = torch.cat((torch.Tensor([0]), sorted_probs, torch.Tensor([1])))\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "        \n",
        "    FAs, FRs = [], []\n",
        "    for prob in sorted_probs:\n",
        "        preds = (probs >= prob) * 1\n",
        "        FA, FR = count_FA_FR(preds, labels)        \n",
        "        FAs.append(FA)\n",
        "        FRs.append(FR)\n",
        "    # plt.plot(FAs, FRs)\n",
        "    # plt.show()\n",
        "\n",
        "    # ~ area under curve using trapezoidal rule\n",
        "    return -np.trapz(FRs, x=FAs)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcEP5cEZqUYl"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cP_pFIsy5p2",
        "outputId": "17bcac7e-655c-4e07-e4b6-f14eff124858"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.energy = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, input):\n",
        "        energy = self.energy(input)\n",
        "        alpha = torch.softmax(energy, dim=-2)\n",
        "        return (input * alpha).sum(dim=-2)\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, config: TaskConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1, out_channels=config.cnn_out_channels,\n",
        "                kernel_size=config.kernel_size, stride=config.stride\n",
        "            ),\n",
        "            nn.Flatten(start_dim=1, end_dim=2),\n",
        "        )\n",
        "\n",
        "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
        "            config.stride[0] + 1\n",
        "        \n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
        "            hidden_size=config.hidden_size,\n",
        "            num_layers=config.gru_num_layers,\n",
        "            dropout=0.1,\n",
        "            bidirectional=config.bidirectional\n",
        "        )\n",
        "\n",
        "        self.attention = Attention(config.hidden_size)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        input = input.unsqueeze(dim=1)\n",
        "        conv_output = self.conv(input).transpose(-1, -2)\n",
        "        gru_output, _ = self.gru(conv_output)\n",
        "        contex_vector = self.attention(gru_output)\n",
        "        output = self.classifier(contex_vector)\n",
        "        return output\n",
        "\n",
        "config = TaskConfig()\n",
        "model = CRNN(config)\n",
        "model"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n",
              "    (1): Flatten(start_dim=1, end_dim=2)\n",
              "  )\n",
              "  (gru): GRU(144, 64, num_layers=2, dropout=0.1)\n",
              "  (attention): Attention(\n",
              "    (energy): Sequential(\n",
              "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmmSFvWaqUYn"
      },
      "source": [
        "def train_epoch(model, opt, loader, log_melspec, device):\n",
        "    model.train()\n",
        "    for i, (batch, labels) in tqdm(enumerate(loader), total=len(loader)):\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        batch = log_melspec(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    return acc"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIeRbn4tqUYo"
      },
      "source": [
        "@torch.no_grad()\n",
        "def validation(model, loader, log_melspec, device):\n",
        "    model.eval()\n",
        "\n",
        "    val_losses, accs, FAs, FRs = [], [], [], []\n",
        "    all_probs, all_labels = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(loader)):\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        batch = log_melspec(batch)\n",
        "\n",
        "        output = model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(output, dim=-1)\n",
        "        loss = F.cross_entropy(output, labels)\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        all_probs.append(probs[:, 1].cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "        val_losses.append(loss.item())\n",
        "        accs.append(\n",
        "            torch.sum(argmax_probs == labels).item() /  # ???\n",
        "            torch.numel(argmax_probs)\n",
        "        )\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        FAs.append(FA)\n",
        "        FRs.append(FR)\n",
        "\n",
        "    # area under FA/FR curve for whole loader\n",
        "    au_fa_fr = get_au_fa_fr(torch.cat(all_probs, dim=0).cpu(), all_labels)\n",
        "    return au_fa_fr"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpyvKwp0k3IU"
      },
      "source": [
        "from collections import defaultdict\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "history = defaultdict(list)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSNW-nZCJ4Q0"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8sVpHNoocgA",
        "outputId": "0e634681-ba15-419f-e6be-13910db73821"
      },
      "source": [
        "config = TaskConfig()\n",
        "model = CRNN(config).to(config.device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "opt = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config.learning_rate,\n",
        "    weight_decay=config.weight_decay\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRNN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n",
            "    (1): Flatten(start_dim=1, end_dim=2)\n",
            "  )\n",
            "  (gru): GRU(144, 64, num_layers=2, dropout=0.1)\n",
            "  (attention): Attention(\n",
            "    (energy): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zedXm9dmINAE",
        "outputId": "3e21581d-94c3-434b-cab6-fb48ab9a7a1f"
      },
      "source": [
        "sum([p.numel() for p in model.parameters()])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70443"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt2kjqC-IobK"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "32oooz4lqUYo",
        "scrolled": false,
        "outputId": "bc577502-f630-4795-fa8a-2068e6f01d04"
      },
      "source": [
        "# TRAIN\n",
        "\n",
        "for n in range(TaskConfig.num_epochs):\n",
        "\n",
        "    train_epoch(model, opt, train_loader,\n",
        "                melspec_train, config.device)\n",
        "\n",
        "    au_fa_fr = validation(model, val_loader,\n",
        "                          melspec_val, config.device)\n",
        "    history['val_metric'].append(au_fa_fr)\n",
        "\n",
        "    clear_output()\n",
        "    plt.plot(history['val_metric'])\n",
        "    plt.ylabel('Metric')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    print('END OF EPOCH', n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "END OF EPOCH 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405/405 [00:57<00:00,  7.07it/s]\n",
            "33it [00:02, 14.38it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t55FUkOGh9pT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}